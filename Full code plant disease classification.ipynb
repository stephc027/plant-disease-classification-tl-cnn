{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing kaggle-hosted dataset \"New Plant Diseases Dataset\" \n",
    "#Requires a Kaggle API Authenification\n",
    "#https://www.kaggle.com/vipoooool/new-plant-diseases-dataset\n",
    "\n",
    "!kaggle datasets download -d vipoooool/new-plant-diseases-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip and download data.  \n",
    "#IMPORTANT: This is a large file of 87K images.  Do not execute if you do not with to download to your current directory\n",
    "from zipfile import ZipFile\n",
    "zf = ZipFile('new-plant-diseases-dataset.zip')\n",
    "zf.extractall()\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will load images for Keras to interpret with the Image Data Generator.\n",
    "#All images are scaled by 1:255 to \"transform every pixel value from range [0,255] -> [0,1]\".  See https://www.linkedin.com/pulse/keras-image-preprocessing-scaling-pixels-training-adwin-jahn/\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "gen_trn = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   fill_mode='nearest')\n",
    "gen_valtst = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and iterate train data\n",
    "train_batch = gen_trn.flow_from_directory('New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train/', class_mode='categorical', batch_size=64, seed=12, target_size = (128,128))\n",
    "# Load and iterate val data\n",
    "val_batch = gen_valtst.flow_from_directory('New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid/', class_mode='categorical', batch_size=64, seed=123,  target_size = (128,128))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine assigned class indices:\n",
    "train_batch.class_indices\n",
    "classes = train_batch.class_indices\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils.vis_utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For L1 regularization import:\n",
    "from keras.regularizers import l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Non-TL model (A):\n",
    "\n",
    "full_model = Sequential()\n",
    "#CNN\n",
    "full_model.add(Convolution2D(32, (3, 3), input_shape = (128,128,3), activation = 'relu'))\n",
    "#Pooling\n",
    "full_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#Flatting to 1D for Dense layers\n",
    "full_model.add(Flatten())\n",
    "full_model.add(Dense(units=512,activation='relu'))\n",
    "full_model.add(Dense(units=256,activation='relu'))\n",
    "#output layer with L1 regularization\n",
    "full_model.add(Dense(units=38,activation='softmax',activity_regularizer=l1(0.001)))\n",
    "\n",
    "#Compile Non-TL Model with Adam optimization\n",
    "full_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(full_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep track of processing time:\n",
    "from time import process_time\n",
    "\n",
    "t1_start = process_time() \n",
    "\n",
    "\n",
    "#Fit model\n",
    "fm_fit = full_model.fit_generator(\n",
    "    train_batch,\n",
    "    steps_per_epoch = 150,\n",
    "    epochs = 10,\n",
    "    validation_data = val_batch,\n",
    "    validation_steps = 50\n",
    "    )\n",
    "\n",
    "\n",
    "# Stop the stopwatch / counter \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", round((t1_stop-t1_start)/360,2),\"m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer Learning\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Convolution2D, MaxPooling2D,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model = Sequential()\n",
    "r_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "r_model.add(Dense(38, activation='softmax'))\n",
    "r_model.compile(optimizer = 'adam', loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#This model is to show how ResNet50 automatically improves accuracy by providing pre-trained image classification info. \n",
    "#However, we did not add any additional learning, just the output softmax layer, so of course the accuracy did not improve much after the first epoch.\n",
    "print(r_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the ResNet50 + output layer model:\n",
    "\n",
    "t1_start = process_time()  \n",
    "r_fit = full_model.fit_generator(\n",
    "    train_batch,\n",
    "    steps_per_epoch = 300,\n",
    "    epochs = 10,\n",
    "    validation_data = val_batch,\n",
    "    validation_steps = 50\n",
    "    )\n",
    "\n",
    "\n",
    "# Stop the stopwatch / counter \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", round((t1_stop-t1_start)/360,2),\"m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now comparing ResNet50 results to EfficientNetB0\n",
    "\n",
    "import efficientnet.keras as efn\n",
    "Using TensorFlow backend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientnetB0 model: method adapted from #adapted from: https://www.kaggle.com/ateplyuk/keras-starter-efficientnet\n",
    "\n",
    "eff_net = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "eff_net.trainable = False\n",
    "\n",
    "\n",
    "from keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from keras import Model\n",
    "\n",
    "x = eff_net.output\n",
    "x = Conv2D(32, (3, 3), input_shape = (128,128,3), activation = 'sigmoid', padding = \"same\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Conv2D(64, (3, 3), input_shape = (128,128,3), activation = 'sigmoid', padding = \"same\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation=tf.nn.swish)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions2a = Dense(38, activation=\"softmax\")(x)\n",
    "model_e_cnn2a = Model(input = eff_net.input, output = predictions2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e_cnn2a.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_e_cnn2a.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN efficientnet model\n",
    "\n",
    "from time import process_time\n",
    "t1_start = process_time()  \n",
    "ecnn_fit = model_e_cnn2a.fit_generator(\n",
    "    train_batch,\n",
    "    steps_per_epoch = 100,\n",
    "    epochs = 10,\n",
    "    validation_data = val_batch,\n",
    "    validation_steps = 10\n",
    "    )\n",
    "\n",
    "\n",
    "# Stop the stopwatch / counter \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", round((t1_stop-t1_start)/360,2),\"m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
